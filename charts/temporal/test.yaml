---
# Source: temporal/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-temporal-default-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: "cGFzc3dvcmQ="
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-temporal-visibility-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: "cGFzc3dvcmQ="
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-temporal-frontend-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal
            replicationFactor: 1
            user: user
        visibility:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal_visibility
            replicationFactor: 1
            user: user
        es-visibility:
            elasticsearch:
                version: "v7"
                url:
                    scheme: "http"
                    host: "elasticsearch-master-headless:9200"
                username: ""
                password: ""
                logLevel: "error"
                indices:
                    visibility: "temporal_visibility_v1_dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: frontend
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 6933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 6934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 6935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 6939
          bindOnIP: "0.0.0.0"
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"
    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""
    archival:
      status: "disabled"

    publicClient:
      hostPort: "release-name-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-temporal-history-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal
            replicationFactor: 1
            user: user
        visibility:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal_visibility
            replicationFactor: 1
            user: user
        es-visibility:
            elasticsearch:
                version: "v7"
                url:
                    scheme: "http"
                    host: "elasticsearch-master-headless:9200"
                username: ""
                password: ""
                logLevel: "error"
                indices:
                    visibility: "temporal_visibility_v1_dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: history
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 6933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 6934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 6935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 6939
          bindOnIP: "0.0.0.0"
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"
    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""
    archival:
      status: "disabled"

    publicClient:
      hostPort: "release-name-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-temporal-matching-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal
            replicationFactor: 1
            user: user
        visibility:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal_visibility
            replicationFactor: 1
            user: user
        es-visibility:
            elasticsearch:
                version: "v7"
                url:
                    scheme: "http"
                    host: "elasticsearch-master-headless:9200"
                username: ""
                password: ""
                logLevel: "error"
                indices:
                    visibility: "temporal_visibility_v1_dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: matching
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 6933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 6934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 6935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 6939
          bindOnIP: "0.0.0.0"
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"
    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""
    archival:
      status: "disabled"

    publicClient:
      hostPort: "release-name-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-temporal-worker-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal
            replicationFactor: 1
            user: user
        visibility:
          cassandra:
            hosts: "release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,release-name-cassandra.default.svc.cluster.local,"
            port: 9042
            password: "{{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}"
            consistency:
              default:
                consistency: local_quorum
                serialConsistency: local_serial
            keyspace: temporal_visibility
            replicationFactor: 1
            user: user
        es-visibility:
            elasticsearch:
                version: "v7"
                url:
                    scheme: "http"
                    host: "elasticsearch-master-headless:9200"
                username: ""
                password: ""
                logLevel: "error"
                indices:
                    visibility: "temporal_visibility_v1_dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: worker
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 6933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 6934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 6935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 6939
          bindOnIP: "0.0.0.0"
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"
    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""
    archival:
      status: "disabled"

    publicClient:
      hostPort: "release-name-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-dynamicconfigmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-temporal-dynamic-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/part-of: temporal
data:
  dynamic_config.yaml: |-
---
# Source: temporal/templates/web-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-temporal-web-config
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: temporal
data:
  config.yml: |
    auth:
      enabled: false
    routing:
      default_to_namespace: null
      issue_report_link: https://github.com/temporalio/web/issues/new/choose
---
# Source: temporal/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.14.3
    release: release-name
    heritage: Helm
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: intra
    port: 7000
    targetPort: 7000
  - name: tls
    port: 7001
    targetPort: 7001
  - name: jmx
    port: 7199
    targetPort: 7199
  - name: cql
    port: 9042
    targetPort: 9042
  - name: thrift
    port: 9160
    targetPort: 9160
  selector:
    app: cassandra
    release: release-name
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "release-name"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  publishNotReadyAddresses: false
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: temporal/templates/admintools-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-admintools
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: admintools
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP 
  ports:
    - port: 22
      targetPort: 22
      protocol: TCP
      name: ssh

  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: admintools
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-frontend
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 7233
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-frontend-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-frontend
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7233
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-matching-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: matching
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-matching
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7235
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: matching
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-history-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: history
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-history
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7234
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: history
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-worker-headless
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-worker
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 7239
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: worker
---
# Source: temporal/templates/web-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-temporal-web
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: web
---
# Source: temporal/templates/admintools-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-admintools
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: admintools
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: admintools
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: admintools
        app.kubernetes.io/part-of: temporal
    spec:
      
      containers:
        - name: admin-tools
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 22
              protocol: TCP
          env:
            # TEMPORAL_CLI_ADDRESS is deprecated, use TEMPORAL_ADDRESS instead
            - name: TEMPORAL_CLI_ADDRESS
              value: release-name-temporal-frontend:7233
            - name: TEMPORAL_ADDRESS
              value: release-name-temporal-frontend:7233
          livenessProbe:
              exec:
                command:
                - ls
                - /
              initialDelaySeconds: 5
              periodSeconds: 5
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-frontend
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: frontend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: frontend
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: d3261c1cd0f9bac74082c4c67e81e2301808431acd4c9b341f64a313aafd64bc
        prometheus.io/job: temporal-frontend
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail http://elasticsearch-master-headless:9200/temporal_visibility_v1_dev 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
      containers:
        - name: temporal-frontend
          image: "temporalio/server:1.22.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_VERSION
              value: "v7"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal_visibility_v1_dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: frontend
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7233
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "release-name-temporal-frontend-config"
        - name: dynamic-config
          configMap:
            name: "release-name-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-history
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: history
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: history
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: history
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: d3261c1cd0f9bac74082c4c67e81e2301808431acd4c9b341f64a313aafd64bc
        prometheus.io/job: temporal-history
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail http://elasticsearch-master-headless:9200/temporal_visibility_v1_dev 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
      containers:
        - name: temporal-history
          image: "temporalio/server:1.22.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_VERSION
              value: "v7"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal_visibility_v1_dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: history
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7234
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "release-name-temporal-history-config"
        - name: dynamic-config
          configMap:
            name: "release-name-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-matching
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: matching
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: matching
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: matching
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: d3261c1cd0f9bac74082c4c67e81e2301808431acd4c9b341f64a313aafd64bc
        prometheus.io/job: temporal-matching
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail http://elasticsearch-master-headless:9200/temporal_visibility_v1_dev 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
      containers:
        - name: temporal-matching
          image: "temporalio/server:1.22.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_VERSION
              value: "v7"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal_visibility_v1_dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: matching
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7235
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "release-name-temporal-matching-config"
        - name: dynamic-config
          configMap:
            name: "release-name-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-worker
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: worker
        app.kubernetes.io/part-of: temporal
      annotations:
        checksum/config: d3261c1cd0f9bac74082c4c67e81e2301808431acd4c9b341f64a313aafd64bc
        prometheus.io/job: temporal-worker
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
    spec:
      
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: check-cassandra-temporal-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal$; do echo waiting for default keyspace to become ready; sleep 1; done;']
        - name: check-cassandra-visibility-schema
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SELECT keyspace_name FROM system_schema.keyspaces" | grep temporal_visibility$; do echo waiting for visibility keyspace to become ready; sleep 1; done;']
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail http://elasticsearch-master-headless:9200/temporal_visibility_v1_dev 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
      containers:
        - name: temporal-worker
          image: "temporalio/server:1.22.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ENABLE_ES
              value: "true"
            - name: ES_SEEDS
              value: "elasticsearch-master-headless"
            - name: ES_PORT
              value: "9200"
            - name: ES_VERSION
              value: "v7"
            - name: ES_SCHEME
              value: "http"
            - name: ES_VIS_INDEX
              value: "temporal_visibility_v1_dev"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              value: ""
            - name: SERVICES
              value: worker
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-temporal-visibility-store
                  key: password
          ports:
            - name: rpc
              containerPort: 7239
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "release-name-temporal-worker-config"
        - name: dynamic-config
          configMap:
            name: "release-name-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-temporal-web
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: web
  template:
    metadata:
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: web
        app.kubernetes.io/part-of: temporal
    spec:
      
      volumes:
        - name: temporal-web-config
          configMap:
            name: release-name-temporal-web-config
      containers:
        - name: temporal-web
          image: "temporalio/ui:2.16.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: TEMPORAL_ADDRESS
              value: "release-name-temporal-frontend.default.svc:7233"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: temporal/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.14.3
    release: release-name
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: cassandra
      release: release-name
  serviceName: release-name-cassandra
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  template:
    metadata:
      labels:
        app: cassandra
        release: release-name
    spec:
      hostNetwork: false
      containers:
      - name: release-name-cassandra
        image: "cassandra:3.11.3"
        imagePullPolicy: "IfNotPresent"
        resources:
          {}
        env:
        - name: CASSANDRA_SEEDS
          value: "release-name-cassandra-0.release-name-cassandra.default.svc.cluster.local"
        - name: MAX_HEAP_SIZE
          value: "512M"
        - name: HEAP_NEWSIZE
          value: "128M"
        - name: CASSANDRA_ENDPOINT_SNITCH
          value: "SimpleSnitch"
        - name: CASSANDRA_CLUSTER_NAME
          value: "cassandra"
        - name: CASSANDRA_DC
          value: "DC1"
        - name: CASSANDRA_RACK
          value: "RAC1"
        - name: CASSANDRA_START_RPC
          value: "false"
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status | grep -E \"^UN\\s+${POD_IP}\"" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        ports:
        - name: intra
          containerPort: 7000
        - name: tls
          containerPort: 7001
        - name: jmx
          containerPort: 7199
        - name: cql
          containerPort: 9042
        - name: thrift
          containerPort: 9160
        volumeMounts:
        - name: data
          mountPath: /var/lib/cassandra
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "exec nodetool decommission"]
      terminationGracePeriodSeconds: 30
      volumes:
      - name: data
        emptyDir: {}
---
# Source: temporal/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        release: "release-name"
        chart: "elasticsearch"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                set -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,"
          - name: discovery.seed_hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: cluster.deprecation_indexing.enabled
            value: "false"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: node.ml
            value: "true"
          - name: node.remote_cluster_client
            value: "true"
        volumeMounts:
---
# Source: temporal/templates/server-pdb.yaml
---
---
# Source: temporal/templates/server-pdb.yaml
---
---
# Source: temporal/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-otgmx-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "release-name-uchxl-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-temporal-schema-setup
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: temporal
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  backoffLimit: 100
  template:
    metadata:
      name: release-name-temporal-schema-setup
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: database
        app.kubernetes.io/part-of: temporal
    spec:
      
      restartPolicy: "OnFailure"
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
        - name: create-default-store
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'create', '-k', 'temporal', '--replication-factor', '1']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: create-visibility-store
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'create', '-k', 'temporal_visibility', '--replication-factor', '1']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
      containers:
        - name: default-schema
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'setup-schema', '-v', '0.0']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: visibility-schema
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'setup-schema', '-v', '0.0']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-temporal-schema-update
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: temporal
  annotations:
    "helm.sh/hook": post-install,pre-upgrade
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  backoffLimit: 100
  template:
    metadata:
      name: release-name-temporal-schema-update
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: database
        app.kubernetes.io/part-of: temporal
    spec:
      
      restartPolicy: "OnFailure"
      initContainers:
        - name: check-cassandra-service
          image: busybox
          command: ['sh', '-c', 'until nslookup release-name-cassandra.default.svc.cluster.local; do echo waiting for cassandra service; sleep 1; done;']
        - name: check-cassandra
          image: "cassandra:3.11.3"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until cqlsh release-name-cassandra.default.svc.cluster.local 9042 -e "SHOW VERSION"; do echo waiting for cassandra to start; sleep 1; done;']
      containers:
        - name: default-schema
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'update-schema', '--schema-dir', '/etc/temporal/schema/cassandra/temporal/versioned']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
        - name: visibility-schema
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['temporal-cassandra-tool', 'update-schema', '--schema-dir', '/etc/temporal/schema/cassandra/visibility/versioned']
          env:
            - name: CASSANDRA_HOST
              value: release-name-cassandra.default.svc.cluster.local
            - name: CASSANDRA_PORT
              value: "9042"
            - name: CASSANDRA_KEYSPACE
              value: temporal_visibility
            - name: CASSANDRA_USER
              value: user
            - name: CASSANDRA_PASSWORD
              value: password
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-temporal-es-index-setup
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.32.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.22.4
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: temporal
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  backoffLimit: 100
  template:
    metadata:
      name: release-name-temporal-es-index-setup
      labels:
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.32.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.22.4
        app.kubernetes.io/component: database
        app.kubernetes.io/part-of: temporal
    spec:
      
      restartPolicy: "OnFailure"
      initContainers:
        - name: check-elasticsearch
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail http://elasticsearch-master-headless:9200 2>&1 > /dev/null; do echo waiting for elasticsearch to start; sleep 1; done;']
      containers:
        - name: create-elasticsearch-index
          image: "temporalio/admin-tools:1.22.4"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c']
          args:
            - 'curl -X PUT --fail http://elasticsearch-master-headless:9200/_template/temporal_visibility_v1_template -H "Content-Type: application/json" --data-binary "@schema/elasticsearch/visibility/index_template_v7.json" 2>&1 &&
              curl -X PUT --fail http://elasticsearch-master-headless:9200/temporal_visibility_v1_dev 2>&1'
